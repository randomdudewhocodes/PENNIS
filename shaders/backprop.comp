#version 450

layout(local_size_x = 16, local_size_y = 16) in;

layout(set = 0, binding = 0) readonly  buffer Weights { float W[]; };
layout(set = 0, binding = 1) readonly  buffer Inputs  { float X[]; };
layout(set = 0, binding = 2) readonly  buffer PreActs { float Z[]; };
layout(set = 0, binding = 3) readonly  buffer Outputs { float A[]; };
layout(set = 0, binding = 4) writeonly buffer dWBatch { float dWb[]; };
layout(set = 0, binding = 5) writeonly buffer dBBatch { float dBb[]; };
layout(set = 0, binding = 6) writeonly buffer dInput  { float dX[]; };
layout(set = 0, binding = 7)           buffer delta   { float dZ[]; };
layout(set = 0, binding = 8) readonly  buffer dOutput { float dA[]; };
layout(set = 0, binding = 9) readonly  buffer Targets { float T[]; };

layout(push_constant) uniform PushConstants {
    uint inSize, outSize, batchSize, actType, isOutput, phase;
} push;

float dActivate(float x)
{
    switch (push.actType)
    {
        case 1: return x > 0 ? 1 : 0;
        case 2:
            float s = 1 / (1 + exp(-x));
            return s * (1 - s);
        case 3:
            float t = tanh(x);
            return 1 - t * t;
        default: return 1;
    }
}

void main()
{
    uint neuron = gl_GlobalInvocationID.x;
    uint batch  = gl_GlobalInvocationID.y;
    
    if(batch >= push.batchSize) return;

    if(push.phase == 0)
    {
        if(neuron >= push.outSize) return;

        uint id = batch * push.outSize + neuron;

        float grad = (push.isOutput == 0 ? dA[id] : 2 * (A[id] - T[id]) / push.batchSize) * dActivate(Z[id]);

        dBb[id] = dZ[id] = grad;

        for (uint i = 0; i < push.inSize; i++)
            dWb[id * push.inSize + i] = grad * X[batch * push.inSize + i];
    }
    else
    {
        if(neuron >= push.inSize) return;

        float sum = 0;

        for (uint i = 0; i < push.outSize; i++)
            sum += W[i * push.inSize + neuron] * dZ[i + batch * push.outSize];
        
        dX[batch * push.inSize + neuron] = sum;
    }
}